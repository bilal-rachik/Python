{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_Testing_and_Quality_Automation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5MN9ZuWyuHk"
      },
      "source": [
        "# Testing and Quality Automation\n",
        "\n",
        "le logiciel est complexe. Peu importe le langage que vous utilisez, les frameworks sur lesquels vous construisez et l'élégance de votre style de codage, il est difficile de vérifier l'exactitude du logiciel en lisant simplement le code. Ce n'est pas seulement parce que les applications non triviales consistent généralement en de grandes quantités de code. C'est aussi parce qu'un logiciel complet est souvent composé de plusieurs couches et repose sur de nombreux composants externes ou interchangeables, tels que des systèmes d'exploitation, des bibliothèques, des bases de données, des caches, des API Web ou des clients utilisés pour interagir avec votre code (navigateurs, par exemple).\n",
        "\n",
        "La complexité des logiciels modernes fait que la vérication de leur exactitude nécessite souvent d'aller au-delà de votre code. Vous devez tenir compte de l'environnement dans lequel votre code s'exécute, des variantes des composants pouvant être remplacés et des manières dont votre code peut interagir. C'est pourquoi les développeurs de logiciels de haute qualité utilisent souvent des techniques de test spéciales qui leur permettent de vérifier rapidement et de manière fiable que le code qu'ils écrivent répond aux critères d'acceptation souhaités.\n",
        "\n",
        "Une autre préoccupation des logiciels complexes est leur maintenabilité. Cela peut être compris comme la facilité du développement continu d'un logiciel. Et le développement ne consiste pas seulement à mettre en œuvre de nouvelles fonctionnalités ou améliorations, mais aussi à diagnostiquer et à résoudre les problèmes qui seront inévitablement découverts en cours de route. Un logiciel maintenable est un logiciel qui nécessite peu d'efforts pour introduire de nouveaux changements et où il y a un faible risque d'introduire de nouveaux défauts lors du changement.\n",
        "\n",
        "Comme vous pouvez probablement le deviner, la maintenabilité est le produit de nombreux aspects logiciels. Les tests automatisés aident bien sûr à réduire le risque de changement en veillant à ce que les cas d'utilisation connus soient correctement couverts par le code existant et futur. Mais il ne suffit pas de garantir que les changements futurs seront faciles à mettre en œuvre. C'est pourquoi les méthodologies de test modernes reposent également sur des mesures et des tests automatisés de la qualité du code pour appliquer des conventions de codage spécifiques, mettre en évidence des fragments de code potentiellement erronés ou rechercher des failles de sécurité.\n",
        "\n",
        "Le paysage des tests modernes est vaste. Il est facile de se perdre dans un océan de méthodologies de test, d'outils, de frameworks, de bibliothèques et d'utilitaires. C'est pourquoi, dans ce chapitre, nous passerons en revue les techniques de test et d'automatisation de la qualité les plus courantes qui sont souvent utilisées par les développeurs Python professionnels. Cela devrait vous donner un bon aperçu de ce qui est généralement possible et vous permettre également de créer votre propre routine de test. Nous aborderons les sujets suivants\n",
        "\n",
        "\n",
        "* Les principes du développement piloté par les tests\n",
        "*  Écrire des tests avec pytest\n",
        "* Automatisation de la qualité\n",
        "* Tests de mutation\n",
        "* Utilitaires de test utiles\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWjgZx4e0kfS"
      },
      "source": [
        "## Les principes du développement piloté par les tests\n",
        "\n",
        "Les tests sont l'un des éléments les plus importants du processus de développement logiciel. Il est si important qu'il existe même une méthodologie de développement logiciel appelée Test-Driven Development (TDD). Il préconise d'écrire les exigences logicielles sous forme de tests comme première (et avant tout) étape du développement de code.\n",
        "\n",
        "\n",
        "Le principe est simple : vous vous concentrez d'abord sur les tests. Utilisez-les pour décrire le comportement du logiciel, le vérifier et rechercher les erreurs potentielles. Ce n'est que lorsque ces tests sont terminés que vous devez procéder à la mise en œuvre réelle pour satisfaire les tests.\n",
        "\n",
        "Le TDD, dans sa forme la plus simple, est un processus itératif qui comprend les étapes suivantes :\n",
        "\n",
        "* 1. Ecrire des tests : les tests doivent refléter la spécification d'une fonctionnalité ou d'une amélioration qui n'a pas encore été implémentée.\n",
        "\n",
        "* 2. Exécuter les tests : à ce stade, tous les nouveaux tests devraient échouer car la fonctionnalité ou l'amélioration n'est pas encore implémentée.\n",
        "\n",
        "* 3. Écrivez une implémentation valide minimale : le code doit être simple mais correct. C'est OK s'il n'a pas l'air élégant ou s'il a des problèmes de performances. L'objectif principal à ce stade devrait être de satisfaire tous les tests écrits à l'étape 1. Il est également plus facile de diagnostiquer les problèmes dans un code simple que dans un code optimisé pour les performances.\n",
        "\n",
        "* 4. Exécuter les tests : à ce stade, tous les tests doivent réussir. Cela inclut les tests nouveaux et préexistants. Si l'un d'entre eux échoue, le code doit être révisé jusqu'à ce qu'il satisfasse aux exigences.\n",
        "\n",
        "* 5. Aiguiser et polir : lorsque tous les tests sont satisfaits, le code peut être progressivement remanié jusqu'à ce qu'il réponde aux normes de qualité souhaitées. C'est le moment de rationaliser, de refactoriser et parfois d'effectuer des optimisations évidentes (si vous avez forcé votre chemin à travers le problème). Après chaque modification, tous les tests doivent être réexécutés pour s'assurer qu'aucune fonctionnalité n'a été interrompue.\n",
        "\n",
        "\n",
        "Ce processus simple vous permet d'étendre votre application de manière itérative sans craindre qu'un nouveau changement ne casse certaines fonctionnalités préexistantes et testées. Il permet également d'éviter une optimisation prématurée et vous guide tout au long du développement avec une série d'étapes simples et de la taille d'une bouchée.\n",
        "\n",
        "\n",
        "TDD ne donnera pas les résultats promis sans une bonne hygiène de travail. C'est pourquoi il est important de suivre quelques principes de base : \n",
        "\n",
        "*  Gardez la taille de l'unité testée petite : En TDD, nous parlons souvent d'unités de code et de tests unitaires. Une unité de code est un simple logiciel autonome qui (de préférence) ne devrait faire qu'une seule chose, et un seul test unitaire devrait exercer une fonction ou une méthode avec un seul ensemble d'arguments. Cela facilite l'écriture des tests, mais favorise également les bonnes pratiques et modèles de développement comme le principe de responsabilité unique et l'inversion du contrôle\n",
        "\n",
        "*  Gardez les tests petits et ciblés : il est presque toujours préférable de créer plusieurs tests petits et simples qu'un test long et élaboré. Chaque test doit vérifier un seul aspect/exigence de la fonctionnalité prévue. Avoir des tests granulaires permet de diagnostiquer plus facilement les problèmes potentiels et d'améliorer la maintenabilité de la suite de tests. Les petits tests identifient mieux les problèmes et sont simplement plus faciles à lire.\n",
        "\n",
        "\n",
        " Si un test repose sur un état spécifique de l'environnement d'exécution, le test lui-même doit s'assurer que toutes les conditions préalables sont remplies. De même, tous les effets secondaires du test doivent être nettoyés après l'exécution. Ces phases de préparation et de nettoyage de chaque test sont également appelées configuration et démontage.\n",
        "\n",
        "Une routine de test rigoureuse et le respect de quelques principes de base sont généralement pris en charge par une bibliothèque ou un cadre de test dédié. Les programmeurs Python ont vraiment de la chance car la bibliothèque standard Python est livrée avec deux modules intégrés créés exactement à des fins de tests automatisés. Ceux-ci sont:\n",
        "\n",
        "* doctest : Un module de test pour tester des exemples de code interactif trouvés dans les docstrings. C'est un moyen pratique de fusionner la documentation avec les tests. doctest est théoriquement capable de gérer les tests unitaires, mais il est plus souvent utilisé pour s'assurer que les extraits de code trouvés dans les docstrings reflètent les bons exemples d'utilisation.\n",
        "\n",
        "Vous pouvez en savoir plus sur doctest dans la documentation ofcielle disponible sur https://docs.python.org/3/library/doctest.html\n",
        "\n",
        "* unittest : Un framework de test complet inspiré de JUnit (un framework de test Java populaire). Il permet l'organisation des tests en cas de test et en suites de tests et fournit des moyens communs de gestion des primitives de configuration et de suppression. unittest est livré avec un lanceur de test intégré qui est capable de découvrir des modules de test dans l'ensemble de la base de code et d'exécuter des sélections de test spécifiques\n",
        "\n",
        "Vous pouvez en savoir plus sur unittest dans la documentation ofcielle disponible sur https://docs.python.org/3/library/unittest.html.\n",
        "\n",
        "\n",
        "Ensemble, ces deux modules peuvent répondre à la plupart des besoins de test des développeurs, même les plus exigeants. Malheureusement, doctest se concentre sur un cas d'utilisation spécifique pour les tests (le test d'exemples de code) et unittest nécessite une assez grande quantité de passe-partout en raison de l'organisation des tests orientés classe. Son patin n'est pas non plus aussi exible qu'il pourrait l'être. C'est pourquoi de nombreux programmeurs professionnels préfèrent utiliser l'un des framework tiers disponibles sur PyPI.\n",
        "\n",
        "Un de ces frameworks est pytest. C'est probablement l'un des frameworks de test Python les meilleurs et les plus matures. Il offre un moyen plus pratique d'organiser les tests sous forme de modules plats avec des fonctions de test (au lieu de classes), mais est également compatible avec la hiérarchie de test basée sur les classes unittest. Il dispose également d'un lanceur de test vraiment supérieur et est livré avec une multitude d'extensions optionnelles.\n",
        "\n",
        "Les avantages ci-dessus de pytest sont la raison pour laquelle nous n'allons pas discuter des détails de l'utilisation d'unittest et de doctest. Ils sont toujours géniaux et utiles, mais pytest est presque toujours un choix meilleur et plus pratique. C'est pourquoi nous allons maintenant discuter d'exemples d'écriture de tests en utilisant pytest comme framework de choix.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC8knIE7GlNk"
      },
      "source": [
        "## Écrire des tests avec pytest\n",
        "\n",
        "Il est maintenant temps de mettre la théorie en pratique. Nous avons déjà les avantages du TDD, nous allons donc essayer de construire quelque chose de simple à l'aide de tests. Nous discuterons de l'anatomie d'un test typique, puis passerons en revue les techniques et outils de test courants qui sont souvent utilisés par les programmeurs Python professionnels. Tout cela sera fait à l'aide du framework de test pytest.\n",
        "\n",
        "Pour ce faire, nous aurons besoin de quelques problèmes à résoudre. Après tout, les tests commencent au tout début du cycle de vie du développement logiciel, lorsque les exigences logicielles sont définies. Dans de nombreuses méthodologies de test, les tests ne sont qu'un moyen natif du code de décrire les exigences logicielles sous forme exécutable.\n",
        "\n",
        "Il est difficile de trouver un seul défi de programmation convaincant qui permettrait de présenter une variété de techniques de test et en même temps s'intégrerait dans un format de livre. C'est pourquoi nous allons plutôt discuter de quelques petits problèmes sans rapport.\n",
        "\n",
        "Du point de vue TDD, l'écriture de tests pour le code existant est bien sûr une approche peu orthodoxe des tests, car l'écriture de tests devrait idéalement précéder l'implémentation et non l'inverse. Mais c'est une pratique connue. Pour les programmeurs professionnels, il n'est pas rare d'hériter d'un logiciel mal testé ou qui n'a pas été testé du tout. Dans une telle situation, si vous souhaitez tester votre logiciel de manière fiable, vous devrez éventuellement faire le travail manquant. Dans notre cas, écrire des tests pour du code préexistant sera aussi une opportunité intéressante pour parler des défis d'écrire des tests après le code.\n",
        "\n",
        "Le premier exemple sera assez simple. Cela nous permettra de comprendre l'anatomie de base d'un test et comment utiliser le lanceur pytest pour découvrir et exécuter des tests. Notre tâche sera de créer une fonction qui :\n",
        "\n",
        "* Accepte un itérable d'éléments et une taille de lot\n",
        "*  Renvoie un itérable de sous-listes où chaque sous-liste est un lot d'éléments consécutifs de la liste source. L'ordre des éléments doit rester le même\n",
        "* Chaque lot a la même taille\n",
        "* Si la liste source n'a pas assez d'éléments pour remplir le dernier lot, ce lot doit être plus court mais jamais vide.\n",
        "\n",
        "Ce serait une fonction relativement petite mais utile. Il pourrait par exemple être utilisé pour traiter de grands flux de données sans avoir besoin de les charger entièrement dans la mémoire de processus. Il pourrait également être utilisé pour distribuer des morceaux de travail afin de traiter des threads séparés ou des travailleurs de processus, comme nous l'avons déja appris.\n",
        "\n",
        "Commençons par écrire le stub de notre fonction pour savoir avec quoi nous travaillons. Il sera nommé batches() et sera hébergé dans un fichier nommé batch.py. La signature peut être la suivante"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElK_RUuAMnwS"
      },
      "source": [
        "from typing import Any, Iterable, List\n",
        "from itertools import islice\n",
        "\n",
        "\n",
        "def batches(iterable: Iterable[Any], batch_size: int) -> Iterable[List[Any]]:\n",
        "    iterator = iter(iterable)\n",
        "\n",
        "    while True:\n",
        "        batch = list(islice(iterator, batch_size))\n",
        "\n",
        "        if not batch:\n",
        "            return\n",
        "\n",
        "        yield batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LPZsU-iMxg2"
      },
      "source": [
        "Nous n'avons pas encore fourni d'implémentation car c'est quelque chose dont nous nous occuperons une fois les tests terminés. On peut voir des annotations de frappe qui font partie du contrat entre la fonction et l'appelant.\n",
        "\n",
        "Une fois cela fait, nous pouvons importer notre fonction dans le module de test pour écrire les tests. La convention courante pour nommer les modules de test est test_<nom-module>.py, où <nom-module> est le nom du module dont nous allons tester le contenu. Créons un fichier nommé test_batch.py.\n",
        "\n",
        "Le premier test fera une chose assez courante : fournir des données d'entrée à la fonction et comparer les résultats. Nous utiliserons une liste littérale simple comme entrée. Ce qui suit est un exemple de code de test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c83jK79CNMMv"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "from batch import batches\n",
        "\n",
        "\n",
        "def test_batch_on_lists():\n",
        "    assert list(batches([1, 2, 3, 4, 5, 6], 1)) == [[1], [2], [3], [4], [5], [6]]\n",
        "    assert list(batches([1, 2, 3, 4, 5, 6], 2)) == [[1, 2], [3, 4], [5, 6]]\n",
        "    assert list(batches([1, 2, 3, 4, 5, 6], 3)) == [[1, 2, 3], [4, 5, 6]]\n",
        "    assert list(batches([1, 2, 3, 4, 5, 6], 4)) == [\n",
        "        [1, 2, 3, 4],\n",
        "        [5, 6],\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzFFX7giNgi4"
      },
      "source": [
        "L'instruction assert est le moyen préféré dans pytest pour tester les pré- et post-conditions des unités de code testées. pytest est capable d'inspecter de telles assertions, de reconnaître leurs exceptions et, grâce à cela, de produire des rapports détaillés des échecs de test sous une forme lisible.\n",
        "\n",
        "Ce qui précède est une structure populaire pour les tests pour les petits services publics et est souvent juste suffisant pour s'assurer qu'ils fonctionnent comme prévu. Pourtant, cela ne reflète pas clairement nos exigences, alors peut-être vaudrait-il la peine de le restructurer un peu.\n",
        "\n",
        "Ce qui suit est un exemple de deux tests supplémentaires qui correspondent plus explicitement à nos exigences prédéfinies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4b1Bm1ZOAlH"
      },
      "source": [
        "def test_batch_order():\n",
        "    iterable = range(100)\n",
        "    batch_size = 2\n",
        "\n",
        "    output = batches(iterable, batch_size)\n",
        "\n",
        "    assert list(chain.from_iterable(output)) == list(iterable)\n",
        "\n",
        "\n",
        "def test_batch_sizes():\n",
        "    iterable = range(100)\n",
        "    batch_size = 2\n",
        "\n",
        "    output = list(batches(iterable, batch_size))\n",
        "\n",
        "    for batch in output[:-1]:\n",
        "        assert len(batch) == batch_size\n",
        "    assert len(output[-1]) <= batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS4eTVSuOzG-"
      },
      "source": [
        "Le test test_batch_order() garantit que l'ordre des éléments dans les lots est le même que dans l'itérable source. Le test test_batch_sizes() s'assure que tous les lots ont la même taille (à l'exception du dernier lot, qui peut être plus court).\n",
        "\n",
        "Nous pouvons également voir un modèle se dérouler dans les deux tests. En fait, de nombreux tests suivent une structure très commune\n",
        "\n",
        "\n",
        "* 1. Configuration : C'est l'étape où les données de test et tous les autres prérequis sont préparés. Dans notre cas, la configuration consiste à préparer des arguments itérables et batch_size.\n",
        "\n",
        "* 2. Exécution : C'est à ce moment-là que l'unité de code testée est mise en service et que les résultats sont enregistrés pour une inspection ultérieure. Dans notre cas, il s'agit d'un appel à la fonction batches().\n",
        "\n",
        "* 3. Validation : Dans cette étape, nous vérifions que l'exigence spécifique est satisfaite en inspectant les résultats de l'exécution de l'unité. Dans notre cas, ce sont toutes les instructions assert utilisées pour vérifier la sortie enregistrée\n",
        "\n",
        "* 4. Nettoyage : il s'agit de l'étape où toutes les ressources susceptibles d'affecter d'autres tests sont libérées ou remises à l'état dans lequel elles se trouvaient avant l'étape de configuration. Nous n'avons pas acquis de telles ressources, donc dans notre cas, cette étape peut être ignorée.\n",
        "\n",
        "Selon le processus de test décrit dans la section Principes du développement piloté par les tests, pour le moment, notre test devrait échouer car nous n'avons pas encore fourni d'implémentation de fonction. Exécutons le pytest runner et voyons comment cela se passe :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFphech_PxMn"
      },
      "source": [
        "    pytest -v\n",
        "\n",
        "\n",
        "le test devera échouer avec trois échecs de test individuels et nous aurons un rapport détaillé de ce qui s'est mal passé. Nous avons le même échec à chaque test. TypeError indique que l'objet NoneType n'est pas itérable et qu'il ne peut donc pas être converti en liste. Cela signifie qu'aucune de nos trois exigences n'a encore été remplie. C'est compréhensible car la fonction batches() ne fait encore rien de significatif.\n",
        "\n",
        "Il est maintenant temps de satisfaire ces tests. L'objectif est de fournir une mise en œuvre minimale de travail. C'est pourquoi nous ne ferons rien d'extraordinaire et fournirons une implémentation simple et naïve basée sur des listes. Jetons un œil à notre première itération\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6kqnVMHQt33"
      },
      "source": [
        "from typing import Any, Iterable, List\n",
        "\n",
        "\n",
        "def batches(iterable: Iterable[Any], batch_size: int) -> Iterable[List[Any]]:\n",
        "    results = []\n",
        "    batch = []\n",
        "\n",
        "    for item in iterable:\n",
        "        batch.append(item)\n",
        "        if len(batch) == batch_size:\n",
        "            results.append(batch)\n",
        "            batch = []\n",
        "\n",
        "    if batch:\n",
        "        results.append(batch)\n",
        "\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSAKCHzuQ0cw"
      },
      "source": [
        "L'idée est simple. Nous créons une liste de résultats et parcourons l'itérable d'entrée, créant activement de nouveaux lots au fur et à mesure. Lorsqu'un lot est plein, nous l'ajoutons à la liste des résultats et en commençons un nouveau. Lorsque nous avons terminé, nous vérifions s'il y a un lot en suspens et l'ajoutons également aux résultats. Puis on retourne les résultats.\n",
        "\n",
        "Il s'agit d'une implémentation assez naïve qui peut ne pas bien fonctionner avec des résultats arbitrairement élevés, mais elle devrait satisfaire nos tests. Exécutons la commande pytest pour voir si cela fonctionne\n",
        "\n",
        "    pytest -v\n",
        "\n",
        "Le résultat du test devrait maintenant être le suivant :\n",
        "\n",
        "\n",
        "    ======================== test session starts ========================\n",
        "    platform darwin -- Python 3.9.2, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 \n",
        "    -- .../Python/.venv/bin/python\n",
        "     cachedir: .pytest_cacherootdir: .../Python/test/01\n",
        "     - Writing tests with pytestcollected 3 itemstest_batch.\n",
        "     py::test_batch_on_lists PASSED                     [ 33%]test_batch.\n",
        "     py::test_batch_order PASSED                        [ 66%]test_batch.\n",
        "     py::test_batch_sizes PASSED                        [100%]\n",
        "     ========================= 3 passed in 0.01s =========================\n",
        "\n",
        "Comme nous pouvons le voir, tous les tests ont réussi. Cela signifie que la fonction batches() satisfait les exigences spécifiées par nos tests. Cela ne signifie pas que le code est complètement exempt de bogues, mais cela nous donne la conance qu'il fonctionne bien dans le domaine des conditions vérifiées par les tests. Plus nous avons de tests et plus ils sont précis, plus nous avons de confiance dans l'exactitude du code.\n",
        "\n",
        "Notre travail n'est pas encore terminé. Nous avons fait une implémentation simple et vérifié qu'elle fonctionne. Nous sommes maintenant prêts à passer à l'étape où nous affinons notre code. L'une des raisons de procéder de cette manière est qu'il est plus facile de repérer les erreurs dans les tests lorsque l'on travaille avec l'implémentation la plus simple possible du code. N'oubliez pas que les tests sont aussi du code, il est donc possible de faire des erreurs dans les tests également.\n",
        "\n",
        "Si l'implémentation d'une unité testée est simple et facile à comprendre, il sera plus facile de vérifier si le code testé est erroné ou s'il y a un problème avec le test lui-même.\n",
        "\n",
        "Le problème évident avec la première itération de notre fonction batches() est qu'elle doit stocker tous les résultats intermédiaires dans la variable de liste de résultats. Si l'argument itérable est suffisamment grand (ou même infini), cela mettra beaucoup de stress sur votre application car elle devra charger toutes les données en mémoire. Une meilleure façon serait de convertir cette fonction en un générateur qui donne des résultats successifs. Cela peut être fait avec seulement un peu de réglage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0ZcSTGeTCkf"
      },
      "source": [
        "def batches(    \n",
        "    iterable: Iterable[Any], batch_size: int\n",
        ") -> Iterable[List[Any]]:    \n",
        "  batch = []    \n",
        "  for item in iterable:        \n",
        "    batch.append(item)        \n",
        "    if len(batch) == batch_size:            \n",
        "      yield batch            \n",
        "      batch = []    \n",
        "  if batch:        \n",
        "    yield batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28cIQ-09T5Hu"
      },
      "source": [
        "L'inverse serait d'utiliser les itérateurs et le module itertools comme dans l'exemple suivant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIyd-YWcT5xV"
      },
      "source": [
        "from itertools import islice\n",
        "\n",
        "def batches(    \n",
        "    iterable: Iterable[Any], batch_size: int\n",
        ") -> Iterable[List[Any]]:    \n",
        "  \n",
        "  iterator = iter(iterable)    \n",
        "  while True:        \n",
        "    batch = list(islice(iterator, batch_size))        \n",
        "    if not batch:            \n",
        "      return        \n",
        "    yield batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDyNEG5iUUIv"
      },
      "source": [
        "C'est ce qui est vraiment génial dans l'approche TDD. Nous sommes maintenant en mesure d'expérimenter et de régler facilement l'implémentation de fonctions existantes avec un risque réduit de casser des éléments. Vous pouvez le tester par vous-même en remplaçant l'implémentation de la fonction batches() par l'une de celles présentées ci-dessus et en exécutant les tests pour voir si elle répond aux exigences définies.\n",
        "\n",
        "Notre exemple était petit et simple à comprendre et nos tests étaient donc faciles à écrire. Mais toutes les unités de code ne seront pas comme ça. Lorsque vous testez des parties de code plus volumineuses ou plus complexes, vous aurez souvent besoin d'outils et de techniques supplémentaires vous permettant d'écrire des tests propres et lisibles. Dans les prochaines sections, nous passerons en revue les techniques de test courantes souvent utilisées par les programmeurs Python et montrerons comment les implémenter à l'aide de pytest. Le premier sera le paramétrage du test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zsq96qhUzdA"
      },
      "source": [
        "## Paramétrage des tests\n",
        "\n",
        "L'utilisation d'une comparaison directe de la sortie de fonction reçue et attendue est une méthode courante pour écrire des tests unitaires courts. Il vous permet d'écrire des tests clairs et condensés. C'est pourquoi nous avons utilisé cette méthode dans notre premier test test_batch_on_lists() dans la section précédente.\n",
        "\n",
        "Un problème avec cette technique est qu'elle rompt avec le schéma classique des étapes de configuration, d'exécution, de vérification et de nettoyage. Vous ne pouvez pas voir clairement quelles instructions préparent le contexte de test, quel appel de fonction constitue l'exécution d'une unité et quelles instructions effectuent la vérification des résultats.\n",
        "\n",
        "L'autre problème est que lorsque le nombre d'échantillons de données d'entrée-sortie augmente, les tests deviennent trop volumineux. Il est plus difficile de les lire et les défaillances indépendantes potentielles ne sont pas correctement isolées. Rappelons le code du test test_batch_on_lists() pour mieux comprendre ce problème"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t58B2-SbVgQq"
      },
      "source": [
        "\n",
        "def test_batch_on_lists():\n",
        "    assert list(batches([1, 2, 3, 4, 5, 6], 1)) == [[1], [2], [3], [4], [5], [6]]\n",
        "    assert list(batches([1, 2, 3, 4, 5, 6], 2)) == [[1, 2], [3, 4], [5, 6]]\n",
        "    assert list(batches([1, 2, 3, 4, 5, 6], 3)) == [[1, 2, 3], [4, 5, 6]]\n",
        "    assert list(batches([1, 2, 3, 4, 5, 6], 4)) == [\n",
        "        [1, 2, 3, 4],\n",
        "        [5, 6],\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJjBK5SJVxbq"
      },
      "source": [
        "Chaque instruction d'assertion est chargée de vérifier une paire d'échantillons d'entrée-sortie. Mais chaque paire peut être construite pour vérifier différentes conditions des exigences initiales. Dans notre cas, les trois premières instructions pourraient vérifier que la taille de chaque lot de sortie est la même. Mais la dernière assertion vérifie que le lot incomplet est également renvoyé si la longueur de l'argument itérable n'est pas divisible par batch_size. L'intention du test n'est pas parfaitement claire car elle enfreint légèrement le principe « garder les tests petits et ciblés »\n",
        "\n",
        "\n",
        "Nous pouvons légèrement améliorer la structure du test en déplaçant la préparation de tous les échantillons vers la partie de configuration distincte du test, puis en itérant sur les échantillons dans la partie d'exécution principale. Dans notre cas, cela peut être fait avec un simple dictionnaire littéral"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N3S2pWQWMAa"
      },
      "source": [
        "def test_batch_with_loop():\n",
        "    iterable = [1, 2, 3, 4, 5, 6]\n",
        "    samples = {\n",
        "        # even batches\n",
        "        1: [[1], [2], [3], [4], [5], [6]],\n",
        "        2: [[1, 2], [3, 4], [5, 6]],\n",
        "        3: [[1, 2, 3], [4, 5, 6]],\n",
        "        # batches with rest\n",
        "        4: [[1, 2, 3, 4], [5, 6]],\n",
        "    }\n",
        "\n",
        "    for batch_size, expected in samples.items():\n",
        "        assert list(batches(iterable, batch_size)) == expected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJNU1_4wWTLr"
      },
      "source": [
        "    Voyez comment un petit changement dans la structure de test nous a permis\n",
        "    d'annoter quels échantillons sont censés vérifier une exigence de fonction\n",
        "    particulière. Nous n'avons pas toujours à utiliser des tests séparés pour\n",
        "    chaque exigence. Rappelez-vous : la praticité bat la pureté\n",
        "\n",
        "Grâce à ce changement, nous avons une séparation plus claire des parties de configuration et d'exécution du test. On peut maintenant dire que l'exécution de la fonction batch() est paramétrée avec le contenu du dictionnaire d'exemples. C'est comme exécuter plusieurs petits tests dans un seul test.\n",
        "\n",
        "\n",
        "Un autre problème lié au test de plusieurs échantillons au sein d'une même fonction de test est que le test peut échouer prématurément. Si la première instruction assert échoue, le test s'arrêtera immédiatement. Nous ne saurons pas si les instructions d'assertion suivantes réussiront ou échoueront jusqu'à ce que nous ayons corrigé la première erreur et que nous soyons capables de poursuivre l'exécution du test. Et avoir un aperçu complet de toutes les défaillances individuelles nous permet souvent de mieux comprendre ce qui ne va pas avec le code testé.\n",
        "\n",
        "\n",
        "Ce problème ne peut pas être facilement résolu dans un test basé sur une boucle. Heureusement, pytest est livré avec un support natif pour le paramétrage des tests sous la forme du décorateur @pytest.mark.parametrize. Cela nous permet de déplacer le paramétrage de l'étape d'exécution d'un test en dehors du corps du test. pytest sera suffisamment intelligent pour traiter chaque ensemble de paramètres d'entrée comme un test \"virtuel\" distinct qui sera exécuté indépendamment des autres échantillons.\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize requiert au moins deux arguments de position :\n",
        "\n",
        "* argnames : il s'agit d'une liste de noms d'arguments que pytest utilisera pour fournir des paramètres de test à la fonction de test en tant qu'arguments. Il peut s'agir d'une chaîne séparée par des virgules ou d'une liste/tuple de chaînes.\n",
        "\n",
        "* argvalues : il s'agit d'un itérable d'ensembles de paramètres pour chaque test individuel. Habituellement, c'est une liste de listes ou un tuple de tuples\n",
        "\n",
        "\n",
        "Nous pourrions réécrire notre dernier exemple pour utiliser le décorateur @pytest.mark.parametrize comme suit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L-vLF3cXaL0"
      },
      "source": [
        "import pytest\n",
        "\n",
        "@pytest.mark.parametrize(\n",
        "    \"batch_size, expected\",\n",
        "    [\n",
        "        # even batches\n",
        "        [1, [[1], [2], [3], [4], [5], [6]]],\n",
        "        [2, [[1, 2], [3, 4], [5, 6]]],\n",
        "        [3, [[1, 2, 3], [4, 5, 6]]],\n",
        "        # batches with rest\n",
        "        [4, [[1, 2, 3, 4], [5, 6]]],\n",
        "    ],\n",
        ")\n",
        "def test_batch_parametrized(batch_size, expected):\n",
        "    iterable = [1, 2, 3, 4, 5, 6]\n",
        "    assert list(batches(iterable, batch_size)) == expected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoQThL4dYVmT"
      },
      "source": [
        "## pytest's fixtures\n",
        "\n",
        "Le terme « fixture » vient de l'ingénierie mécanique et électronique. Il s'agit d'un dispositif physique qui peut prendre la forme d'une pince ou d'une poignée qui maintient le matériel testé dans une position et une configuration fixes (d'où le nom « fixture ») pour lui permettre d'être testé de manière cohérente dans un environnement.\n",
        "\n",
        "Les fixture de test de logiciels ont un objectif similaire. Ils simulent une configuration d'environnement fixe qui essaie d'imiter l'utilisation réelle du composant logiciel testé. Les fixture peuvent être n'importe quoi, des objets spécifiques utilisés comme arguments d'entrée, via des configurations de variables d'environnement, aux ensembles de données stockées dans une base de données distante qui sont utilisés pendant la procédure de test.\n",
        "\n",
        "Dans pytest, une fixture est un morceau de code de configuration et/ou de démontage réutilisable qui peut être fourni en tant que dépendance aux fonctions de test. pytest dispose d'un mécanisme d'injection de dépendances intégré qui permet d'écrire des suites de tests modulaires et évolutives.\n",
        "\n",
        "\n",
        "Pour créer une fixture pytest, vous devez définir une fonction nommée et la décorer avec le décorateur @pytest.fixture comme dans l'exemple suivant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6khXjMvRZq8N"
      },
      "source": [
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "def dependency():    \n",
        "  return \"fixture value\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aSo2hbiZzZ-"
      },
      "source": [
        "pytest exécute les fonctions fixture avant l'exécution du test. La valeur de retour de la fonction fixture (ici \"fixture value\") sera fournie à la fonction test en tant qu'argument d'entrée. Il est également possible de fournir à la fois le code de configuration et le code de nettoyage dans la même fonction de montage en utilisant la syntaxe de générateur suivante:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQi69WIzaKpV"
      },
      "source": [
        "@pytest.fixture\n",
        "def dependency_as_generator():\n",
        "    # setup code\n",
        "    yield \"fixture value\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOwpfMXMaSsP"
      },
      "source": [
        "Lorsque la syntaxe du générateur est utilisée, pytest obtiendra la valeur produite de la fonction fixture et la maintiendra suspendue jusqu'à ce que le test termine son exécution. Une fois le test terminé, pytest reprendra l'exécution de toutes les fonctions fixture utilisées juste après l'instruction yield, quel que soit le résultat du test (échec ou succès). Cela permet un nettoyage pratique et fiable de l'environnement de test.\n",
        "\n",
        "\n",
        "Pour utiliser un fixture dans un test, vous devez utiliser son nom comme argument d'entrée de la fonction de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBCjcERZcDSm"
      },
      "source": [
        "def test_fixture(dependency):\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2PVVmWncJQV"
      },
      "source": [
        "Lors du démarrage d'un programme d'exécution pytest, pytest collectera toutes les utilisations de fixture en inspectant les signatures des fonctions de test et en faisant correspondre les noms avec les fonctions de fixture disponibles. Par défaut, pytest découvrira les appareils et effectuera leur résolution de nom de plusieurs manières :\n",
        "\n",
        "\n",
        "\n",
        "* Appareils locaux : les tests peuvent utiliser tous les appareils disponibles à partir du même module dans lequel ils sont définis. Il peut s'agir d'appareils importés dans le même module. Les montages locaux ont toujours la priorité sur les montages partagés. \n",
        "\n",
        "* Les montages partagés : les tests peuvent utiliser les montages disponibles dans le module de conftest stockés dans le même répertoire que le module de test ou l'un de ses répertoires parents. Une suite de tests peut avoir plusieurs modules de conftest. Les appareils de conftest qui sont plus proches dans la hiérarchie des répertoires ont la priorité sur ceux qui sont plus loin dans la hiérarchie des répertoires. Les xtures partagées ont toujours la priorité sur les xtures de plugin.\n",
        "\n",
        "• Plugin fixtures : les plugins pytest peuvent fournir leurs propres fixtures. Ces noms de luminaires seront appariés en dernier.\n",
        "\n",
        "\n",
        "Enfin et surtout, les fixture peuvent être associés à des étendues spécifiques qui décident de la durée de vie des valeurs d'appareil. Ces étendues sont extrêmement importantes pour les fixtures implémentées en tant que générateurs car elles déterminent quand le code de nettoyage est exécuté. Il y a cinq portées disponibles :\n",
        "\n",
        "* Portée « fonction » : c'est la portée par défaut. Une fonction fxture avec la portée « fonction » sera exécutée une fois pour chaque test individuel et sera détruite par la suite.\n",
        "\n",
        "* Portée « classe » : cette portée peut être utilisée pour les méthodes de test écrites dans le style xUnit (basé sur le module unittest ). Les Fixture avec cette étendue sont détruits après le dernier test dans une classe de test.\n",
        "\n",
        "* Étendue « module » : les appareils avec cette étendue sont détruits après le dernier test dans le module de test.\n",
        "\n",
        "* Étendue « package » : les fixture avec cette étendue sont détruits après le dernier test dans le package de test donné (collection de modules de test).\n",
        "\n",
        "\n",
        "*  Portée \"session\": C'est une sorte de portée globale. Les fixture avec cette portée vivent pendant toute l'exécution du coureur et sont détruits après le dernier test\n",
        "\n",
        "\n",
        "Différentes étendues d'installations peuvent être utilisées pour optimiser l'exécution du test, car une configuration d'environnement spécifique peut parfois prendre un temps considérable à s'exécuter. Si de nombreux tests peuvent réutiliser en toute sécurité la même configuration, il peut être raisonnable d'étendre la portée par défaut de la \"fonction\" à \"module\", \"package\" ou même \"session\"\n",
        "\n",
        "\n",
        "De plus, les fixture de \"session\" peuvent être utilisés pour effectuer une configuration globale pour l'ensemble du test ainsi qu'un nettoyage global. C'est pourquoi ils sont souvent utilisés avec l'indicateur autouse=True, qui marque une fixture comme une dépendance automatique pour un groupe de tests donné. La portée des autouse fixture à usage automatique est la suivante:\n",
        "\n",
        "*  Au niveau du module pour le module de test fixture : si un fixture avec le drapeau d'utilisation automatique est inclus dans le module de test (un module avec le préfixe de test), il sera automatiquement marqué comme une dépendance de chaque test dans ce module\n",
        "\n",
        "* Au niveau du package pour le module de test de conftest fixture : si un fixture avec le drapeau autouse est inclus dans un module de conftest d'un répertoire de test donné, il sera automatiquement marqué comme une dépendance de chaque test dans chaque module de test au sein du même annuaire. Cela inclut également les sous-répertoires.\n",
        "\n",
        "\n",
        "La meilleure façon d'apprendre à utiliser des fixtures sous diverses formes est par l'exemple. Nos tests pour la fonction batch() de la section précédente étaient assez simples et ne nécessitaient donc pas l'utilisation intensive de fixtures. Les correctifs sont particulièrement utiles si vous devez fournir une initialisation d'objet complexe ou l'état de configuration de composants logiciels externes tels que des services distants ou des bases de données. Interfaces, modèles et modularité, nous avons présenté des exemples de code pour le suivi du nombre de pages vues avec des backends de stockage enfichables, et l'un de ces exemples utilisait Redis comme implémentation de stockage. Tester ces backends serait un cas d'utilisation parfait pour pytest fixtures, rappelons donc l'interface commune de la classe de base abstraite ViewsStorageBackend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edcSKJvEYkBR"
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "class ViewsStorageBackend(ABC):\n",
        "    @abstractmethod\n",
        "    def increment(self, key: str):\n",
        "        ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def most_common(self, n: int) -> Dict[str, int]:\n",
        "        ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA02exyYYo4S"
      },
      "source": [
        "Les classes de base abstraites ou tout autre type d'implémentation d'interface, comme les sous-classes de protocole, sont en fait très utiles pour les tests. Ils vous permettent de vous concentrer sur le comportement de la classe plutôt que sur l'implémentation.\n",
        "\n",
        "Si nous souhaitons tester le comportement de n'importe quelle implémentation de ViewsStorageBackend, nous pouvons tester plusieurs choses : \n",
        "\n",
        "*  Si nous recevons un backend de stockage vide, la méthode most_common() renverra un dictionnaire vide \n",
        "\n",
        "* Si nous incrémentons un nombre de pages pour différentes clés et demander un nombre de clés les plus courantes supérieur ou égal au nombre de clés incrémentées, nous recevrons tous les décomptes suivis\n",
        "\n",
        "*  Si nous incrémentons un nombre de décomptes de pages pour différentes clés et demandons un nombre de clés les plus courantes supérieur ou égal au nombre de clés incrémentées, nous recevrons un ensemble raccourci des éléments les plus courants\n",
        "\n",
        "Nous commencerons par des tests, puis passerons en revue la mise en œuvre réelle de la fixture. La première fonction de test pour le backend de stockage vide sera vraiment simple\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWqO9aGCZbRh"
      },
      "source": [
        "import pytest\n",
        "import random\n",
        "\n",
        "@pytest.mark.parametrize(\"n\", [0] + [random.randint(0, 100) for _ in range(5)])\n",
        "def test_empty_backend(backend: ViewsStorageBackend, n: int):\n",
        "    assert backend.most_common(n) == {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skh5z11JaOrh"
      },
      "source": [
        "Ce test ne nécessite aucune configuration élaborée. Nous pourrions utiliser un ensemble statique de n paramètres d'argument, mais un paramétrage supplémentaire avec des valeurs aléatoires ajoute une touche agréable au test. L'argument backend est une déclaration d'utilisation d'une fixture qui sera résolue par pytest lors de l'exécution du test.\n",
        "\n",
        "Le deuxième test pour obtenir un ensemble complet de nombres d'incréments nécessitera une configuration et une exécution plus détaillées"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl9_lRKFajke"
      },
      "source": [
        "def test_increments_all(backend: ViewsStorageBackend):\n",
        "    increments = {\n",
        "        \"key_a\": random.randint(1, 10),\n",
        "        \"key_b\": random.randint(1, 10),\n",
        "        \"key_c\": random.randint(1, 10),\n",
        "    }\n",
        "\n",
        "    for key, count in increments.items():\n",
        "        for _ in range(count):\n",
        "            backend.increment(key)\n",
        "\n",
        "    assert backend.most_common(len(increments)) == increments\n",
        "    assert backend.most_common(len(increments) + 1) == increments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yry4FnuQa-Ba"
      },
      "source": [
        "Le test commence par la déclaration d'une variable de dictionnaire littérale avec les incréments souhaités. Cette configuration simple sert à deux fins : la variable d'incréments guide l'étape d'exécution ultérieure et sert également de données de validation pour deux assertions de vérification. Comme dans le test précédent, nous nous attendons à ce que l'argument backend soit fourni par un fixture pytest.\n",
        "\n",
        "Le dernier test est assez similaire au précédent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfKTLnoQbQro"
      },
      "source": [
        "def test_increments_top(backend: ViewsStorageBackend):\n",
        "    increments = {\n",
        "        \"key_a\": random.randint(1, 10),\n",
        "        \"key_b\": random.randint(1, 10),\n",
        "        \"key_c\": random.randint(1, 10),\n",
        "        \"key_d\": random.randint(1, 10),\n",
        "    }\n",
        "\n",
        "    for key, count in increments.items():\n",
        "        for _ in range(count):\n",
        "            backend.increment(key)\n",
        "\n",
        "    assert len(backend.most_common(1)) == 1\n",
        "    assert len(backend.most_common(2)) == 2\n",
        "    assert len(backend.most_common(3)) == 3\n",
        "\n",
        "    top2_values = backend.most_common(2).values()\n",
        "    assert list(top2_values) == (sorted(increments.values(), reverse=True)[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BoutG8obeiw"
      },
      "source": [
        "Les étapes de configuration et d'exécution sont similaires à celles utilisées dans la fonction de test test_increments_all(). Si nous n'écrivions pas de tests, nous envisagerions probablement de déplacer ces étapes vers des fonctions réutilisables séparées. Mais ici, cela aurait probablement un impact négatif sur la lisibilité. Les tests doivent rester indépendants, donc un peu de redondance ne fait souvent pas de mal si cela permet des tests clairs et explicites. Cependant, ce n'est pas une règle bien sûr et nécessite toujours un jugement personnel.\n",
        "\n",
        "Puisque tous les tests sont écrits, il est temps de fournir un montage. Dans le guisz précident, Interfaces, modèles et modularité, nous avons inclus deux implémentations de backends : CounterBackend et RedisBackend. En fin de compte, nous aimerions utiliser le même ensemble de tests pour les deux backends de stockage. Nous y arriverons éventuellement, mais pour l'instant, supposons qu'il n'y a qu'un seul backend. ça simplifiera un peu les choses.\n",
        "\n",
        "Supposons pour l'instant que nous testons uniquement RedisBackend. C'est nettement plus complexe que CounterBackend, nous nous amuserons donc plus à le faire. Nous pourrions n'écrire qu'une seule version de backend, mais pytest nous permet d'avoir des versions modulaires, alors voyons comment cela fonctionne. Nous commencerons par ce qui suit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Qpjqjxcklq"
      },
      "source": [
        "from collections import Counter\n",
        "from typing import Dict\n",
        "from redis import Redis\n",
        "\n",
        "\n",
        "class CounterBackend(ViewsStorageBackend):\n",
        "    def __init__(self):\n",
        "        self._counter = Counter()\n",
        "\n",
        "    def increment(self, key: str):\n",
        "        self._counter[key] += 1\n",
        "\n",
        "    def most_common(self, n: int) -> Dict[str, int]:\n",
        "        return dict(self._counter.most_common(n))\n",
        "\n",
        "\n",
        "class RedisBackend(ViewsStorageBackend):\n",
        "    def __init__(self, redis_client: Redis, set_name: str):\n",
        "        self._client = redis_client\n",
        "        self._set_name = set_name\n",
        "\n",
        "    def increment(self, key: str):\n",
        "        self._client.zincrby(self._set_name, 1, key)\n",
        "\n",
        "    def most_common(self, n: int) -> Dict[str, int]:\n",
        "        return {\n",
        "            key.decode(): int(value)\n",
        "            for key, value in self._client.zrange(\n",
        "                self._set_name,\n",
        "                0,\n",
        "                n - 1,\n",
        "                desc=True,\n",
        "                withscores=True,\n",
        "            )\n",
        "        }\n",
        "\n",
        "        \n",
        "@pytest.fixture\n",
        "def redis_backend(redis_client: Redis):\n",
        "    set_name = \"test-page-counts\"\n",
        "    redis_client.delete(set_name)\n",
        "\n",
        "    return RedisBackend(redis_client=redis_client, set_name=set_name)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENf5kFPEdVAa"
      },
      "source": [
        "redis_client.delete(set_name) supprime la clé dans le magasin de données Redis si elle existe. Nous utiliserons la même clé dans l'initialisation RedisBackend. La clé Redis sous-jacente qui stocke tous nos incréments sera créée lors de la première modification de stockage, nous n'avons donc pas à nous soucier des clés inexistantes. De cette façon, nous nous assurons qu'à chaque fois qu'un fixture est initialisé, le backend de stockage est complètement vide. La portée par défaut de la session fixture est \"fonction\", et cela signifie que chaque test utilisant cette fixture recevra un backend vide\n",
        "\n",
        "\n",
        "    Redis ne fait pas partie de la plupart des distributions système, vous\n",
        "    devrez donc probablement l'installer vous-même. La plupart des\n",
        "    distributions Linux l'ont disponible sous le nom du package redis-server\n",
        "    dans leurs référentiels de packages. Vous pouvez également utiliser Docker\n",
        "    et Docker Compose.\n",
        "\n",
        "Vous avez peut-être remarqué que nous n'avons pas instancié le client Redis dans le backend() fixture et l'avons plutôt spécifié comme argument d'entrée des fonctions fixture.\n",
        "\n",
        "Le mécanisme d'injection de dépendance dans pytest couvre également les fonctions fixture. Cela signifie que vous pouvez demander d'autres luminaires à fixture d'une fixture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ddNogeMeKx5"
      },
      "source": [
        "@pytest.fixture(scope=\"session\")\n",
        "def redis_client():\n",
        "    return Redis(host=\"localhost\", port=6379)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suGiyBxLemMS"
      },
      "source": [
        "Pour éviter de trop compliquer les choses, nous avons simplement codé en dur les valeurs des arguments de l'hôte et du port Redis. Grâce à la modularité ci-dessus, il sera plus facile de remplacer ces valeurs globalement si jamais vous décidez d'utiliser une adresse distante à la place.\n",
        "\n",
        "\n",
        "Nous savons que les tests doivent rester indépendants. Néanmoins, nos trois tests n'ont référencé que la classe de base abstraite ViewsStorageBackend. Ils doivent donc toujours être les mêmes, quelle que soit la mise en œuvre réelle des backends de stockage testés. Ce que nous devons faire est de trouver un moyen de définir une structure paramétrée qui nous permettra de répéter le même test sur différentes implémentations backend.\n",
        "\n",
        "Le paramétrage des fonctions de montage est un peu différent du paramétrage des fonctions de test. Le décorateur @pytest.fixture accepte une valeur de mot-clé params facultative qui accepte un itérable de paramètres fixture. Une fixture avec le mot-clé params doit également déclarer l'utilisation d'une requête intégrée spéciale fixture qui, entre autres, permet d'accéder au paramètre fixture courant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkvazYBBf5X4"
      },
      "source": [
        "import pytest\n",
        "\n",
        "@pytest.fixture(params=[param1, param2, ...])\n",
        "\n",
        "def parmetrized_fixture(request: pytest.FixtureRequest):    \n",
        "  return request.param"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiZQSgdwf3by"
      },
      "source": [
        "Nous pouvons utiliser le xture paramétré et la méthode request.getfixturevalue() pour charger dynamiquement un xture en fonction d'un paramètre xture. L'ensemble révisé et complet d'appareils pour nos fonctions de test peut maintenant se présenter comme suit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W48OcKe1fuSL"
      },
      "source": [
        "@pytest.fixture\n",
        "def counter_backend():\n",
        "    return CounterBackend()\n",
        "\n",
        "\n",
        "@pytest.fixture(scope=\"session\")\n",
        "def redis_client():\n",
        "    return Redis(host=\"localhost\", port=6379)\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def redis_backend(redis_client: Redis):\n",
        "    set_name = \"test-page-counts\"\n",
        "    redis_client.delete(set_name)\n",
        "\n",
        "    return RedisBackend(redis_client=redis_client, set_name=set_name)\n",
        "\n",
        "\n",
        "@pytest.fixture(params=[\"redis_backend\", \"counter_backend\"])\n",
        "def backend(request):\n",
        "    return request.getfixturevalue(request.param)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TyFA3QugbBS"
      },
      "source": [
        "Si vous exécutez maintenant la même suite de tests avec un nouvel ensemble de fixture, vous verrez que le nombre de tests exécutés vient de doubler. \n",
        "\n",
        "\n",
        "Grâce à l'utilisation intelligente des fixtures, nous avons réduit la quantité de code de test sans affecter la lisibilité du test. Nous pourrions également réutiliser les mêmes fonctions de test pour vérifier des classes qui devraient avoir le même comportement mais des implémentations différentes. Ainsi, chaque fois que les exigences changent, nous pouvons être sûrs que nous serons en mesure de reconnaître les différences entre les classes d'une même interface\n",
        "\n",
        "    Vous devez être prudent lors de la conception de vos fixture, car la\n",
        "    surutilisation de l'injection de dépendances peut rendre plus difficile la\n",
        "    compréhension de l'ensemble de la suite de tests. Les fonctions du\n",
        "    fixture doivent rester simples et bien documentées\n",
        "\n",
        "\n",
        "L'utilisation de fixtures pour fournir une connectivité à des services externes comme Redis est pratique car l'installation de Redis est assez simple et ne nécessite aucune configuration personnalisée pour l'utiliser à des fins de test. Mais parfois, votre code utilisera un service ou une ressource distante que vous ne pouvez pas facilement fournir dans l'environnement de test ou sur lequel vous ne pouvez pas effectuer de tests sans apporter de modifications destructives. Cela peut être assez courant lorsque vous travaillez avec des API Web tierces, du matériel ou des bibliothèques fermées"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgTnZlDuhRlB"
      },
      "source": [
        "## Utiliser Fake\n",
        "\n",
        "L'écriture de tests unitaires suppose que vous puissiez isoler l'unité de code qui est testée. Les tests alimentent généralement la fonction ou la méthode avec des données et vérifient sa valeur de retour et/ou les effets secondaires de son exécution. Il s'agit principalement de s'assurer que :\n",
        "\n",
        "* Les tests concernent une partie atomique de l'application, qui peut être une fonction, une méthode, une classe ou une interface\n",
        "\n",
        "* Les tests fournissent des résultats déterministes et reproductibles\n",
        "\n",
        "Parfois, l'isolement approprié de la composante du programme n'est pas évident ou facile à réaliser. Dans la section précédente, nous avons discuté de l'exemple d'une suite de tests qui, entre autres, vérifiait un morceau de code qui interagissait avec un magasin de données Redis. Nous avons fourni la connectivité à Redis à l'aide d'un fixture pytest et nous avons vu que ce n'était pas si difficile. Mais avons-nous testé uniquement notre code, ou avons-nous également testé le comportement de Redis ?\n",
        "\n",
        "Dans ce cas particulier, inclure la connectivité à Redis était un choix pragmatique. Notre code n'a fait qu'un peu de travail et a laissé la plupart des tâches lourdes au moteur de stockage externe. Cela ne pourrait pas fonctionner correctement si Redis ne fonctionnait pas correctement. Afin de tester l'ensemble de la solution, nous avons dû tester l'intégration de notre code et du data store Redis. De tels tests sont souvent appelés tests d'intégration et sont couramment utilisés dans les logiciels de test qui reposent fortement sur des composants externes.\n",
        "\n",
        "Mais des tests d'intégration sûrs ne sont pas toujours possibles. Tous les services que vous utiliserez ne seront pas aussi faciles à démarrer localement que Redis. Parfois, vous aurez affaire à ces composants \"spéciaux\" qui ne peuvent pas être répliqués en dehors d'une utilisation de production ordinaire\n",
        "\n",
        "Dans de tels cas, vous devrez remplacer la dépendance par un faux objet qui simule un composant réel\n",
        "\n",
        "Pour mieux comprendre les cas d'utilisation typiques de l'utilisation de Fake dans les tests, considérons l'histoire imaginaire suivante : Nous développons une application évolutive qui offre à nos clients la possibilité de suivre le nombre de pages sur leurs sites en temps réel. Contrairement à nos concurrents, nous proposons une solution hautement disponible et évolutive avec une latence très faible et la possibilité de fonctionner avec des résultats cohérents dans de nombreux centres de données à travers le monde. La pierre angulaire de notre produit est une petite classe de compteur du module backends.py\n",
        "\n",
        "Disposer d'une carte de hachage distribuée hautement disponible (le type de données que nous avons utilisé dans Redis) qui garantirait une faible latence dans une configuration multirégionale n'est pas quelque chose de trivial. Il est certain qu'une instance Redis ne fera pas ce que nous annonçons à nos clients. Heureusement, un fournisseur de cloud computing, ACME Corp, nous a contacté récemment, proposant l'un de leurs derniers produits bêta. Il s'appelle ACME Global HashMap Service et il fait exactement ce que nous voulons. Mais il y a un hic : il est encore en version bêta, et donc ACME Corp, par sa politique, ne fournit pas encore d'environnement sandbox que nous pouvons utiliser à des fins de test. De plus, pour des raisons juridiques peu claires, nous ne pouvons pas utiliser le point de terminaison du service de production dans nos pipelines de tests automatisés.\n",
        "\n",
        "Alors, que pourrions-nous faire? Notre code grandit chaque jour. La classe AcmeStorageBackend prévue aura probablement le code supplémentaire qui gère la journalisation, la télémétrie, le contrôle d'accès et bien d'autres choses sophistiquées. Nous voulons absolument pouvoir le tester à fond. Par conséquent, nous avons décidé d'utiliser un Fake substitut du SDK ACME Corp que nous étions censés intégrer dans notre produit\n",
        "\n",
        "Le SDK Python d'ACME Corp se présente sous la forme du package acme_sdk. Il comprend entre autres les deux interfaces suivantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zeb_WEMalEMv"
      },
      "source": [
        "from typing import Dict\n",
        "\n",
        "\n",
        "class AcmeSession:\n",
        "    def __init__(self, tenant: str, token: str): ...\n",
        "\n",
        "\n",
        "class AcmeHashMap:\n",
        "    def __init__(self, acme_session: AcmeSession): ...\n",
        "\n",
        "    def incr(self, key: str, amount):\n",
        "        \"\"\"Increments any key by specific amount\"\"\"\n",
        "\n",
        "    def atomic_incr(self, key: str, amount):\n",
        "        \"\"\"Increments any key by specific amount atomically\"\"\"\n",
        "\n",
        "    def top_keys(self, count: int) -> Dict[str, int]:\n",
        "        \"\"\"Returns keys with top values\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdjpqVcMlOwm"
      },
      "source": [
        "La session AcmeSession est un objet qui encapsule la connexion aux services ACME Corp, et AcmeHashMap est le client de service que nous voulons utiliser. Nous utiliserons très probablement la méthode atomic_incr() pour incrémenter le nombre de pages vues. top_keys() nous donnera la possibilité d'obtenir les pages les plus courantes.\n",
        "\n",
        "Pour construire un fake, nous devons simplement défnir une nouvelle classe qui a une interface compatible avec notre utilisation d'AcmeHashMap. Nous pouvons adopter une approche pragmatique et implémenter uniquement les classes et méthodes que nous prévoyons d'utiliser. L'implémentation minimale d'AcmeHashMapFake pourrait être la suivante"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srNclmY2lon-"
      },
      "source": [
        "from collections import Counter\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "class AcmeHashMapFake:\n",
        "    def __init__(self):\n",
        "        self._counter = Counter()\n",
        "\n",
        "    def atomic_incr(self, key: str, amount):\n",
        "        self._counter[key] += amount\n",
        "\n",
        "    def top_keys(self, count: int) -> Dict[str, int]:\n",
        "        return dict(self._counter.most_common(count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICWl3mFFlw2u"
      },
      "source": [
        "Nous pouvons utiliser AcmeHashMapFake pour fournir une nouvelle version dans la suite de tests existante pour nos backends de stockage. Supposons que nous ayons une classe AcmeBackend dans le module backends qui utilise l'instance AcmeHashMapFake comme seul argument d'entrée. Nous pourrions alors fournir les deux fonctions de montage pytest suivantes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBIbKR_2mNiJ"
      },
      "source": [
        "from backends import AcmeBackend\n",
        "from acme_fakes import AcmeHashMapFake\n",
        "\n",
        "@pytest.fixturedef acme_client():    \n",
        "return AcmeHashMapFake()\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def acme_backend(acme_client):    \n",
        "  return AcmeBackend(acme_client)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYkUs3sPmcbO"
      },
      "source": [
        "Diviser la configuration en deux fixture nous prépare à ce qui pourrait arriver dans un proche avenir. Lorsque nous mettrons enfin la main sur l'environnement sandbox ACME Corp, nous n'aurons à modifier qu'une seule fixture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbXfpa7Ymn9M"
      },
      "source": [
        "from acme_sdk import AcmeHashMap, AcmeSession\n",
        "\n",
        "@pytest.fixturedef acme_client():\n",
        "    return AcmeHashMap(AcmeSession(..., ...))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKt1oZNUmxO1"
      },
      "source": [
        "Pour résumer, les fakes fournissent le comportement équivalent pour un objet que nous ne pouvons pas construire lors d'un test ou que nous ne voulons tout simplement pas construire. Ceci est particulièrement utile pour les situations où vous devez communiquer avec des services externes ou accéder à des ressources distantes. En internalisant ces ressources, vous gagnez un meilleur contrôle de l'environnement de test et êtes ainsi en mesure de mieux isoler l'unité de code testée\n",
        "\n",
        "Construire des contrefaçons personnalisées peut devenir une tâche fastidieuse si vous devez en construire beaucoup. Heureusement, la bibliothèque Python est livrée avec le module unittest.mock, qui peut être utilisé pour automatiser la création de faux objets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69aeJ_x-nEWe"
      },
      "source": [
        "## Mocks et le module unittest.mock\n",
        "\n",
        "Les objets Mocks sont de faux objets génériques qui peuvent être utilisés pour isoler le code testé. Ils automatisent le processus de construction de l'entrée et de la sortie du faux objet. Il y a un plus grand niveau d'utilisation d'objets mock dans les langages à typage statique, où le patch singe est plus difficile, mais ils sont toujours utiles en Python pour raccourcir le code qui imite les API externes.\n",
        "\n",
        "Il existe de nombreuses bibliothèques mocks disponibles en Python, mais la plus reconnue est unittest.mock, qui est fournie dans la bibliothèque standard.\n",
        "\n",
        "Les mocks peuvent presque toujours être utilisées à la place de faux objets personnalisés. Ils sont particulièrement utiles pour falsifier des composants et des ressources externes sur lesquels nous n'avons pas un contrôle total pendant le test. Ils sont également un utilitaire indispensable lorsque nous devons aller à l'encontre du principe TDD premier, c'est-à-dire lorsque nous devons écrire des tests après que l'implémentation a été écrite.\n",
        "\n",
        "\n",
        "Nous avons déjà discuté de l'exemple de falsification de la couche de connectivité à la ressource externe dans la section précédente. Nous allons maintenant examiner de plus près une situation où nous devons écrire un test pour un morceau de code déjà existant qui n'a pas encore de test.\n",
        "\n",
        "Disons que nous avons la fonction send() suivante qui est censée envoyer des e-mails via le protocole SMTP\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmNfFYEboasX"
      },
      "source": [
        "import smtplib\n",
        "import email.message\n",
        "\n",
        "\n",
        "def send(\n",
        "    sender, to,\n",
        "    subject='None',\n",
        "    body='None',\n",
        "    server='localhost'\n",
        "):\n",
        "    \"\"\"sends a message.\"\"\"\n",
        "    message = email.message.Message()\n",
        "    message['To'] = to\n",
        "    message['From'] = sender\n",
        "    message['Subject'] = subject\n",
        "    message.set_payload(body)\n",
        "\n",
        "    client = smtplib.SMTP(server)\n",
        "    try:\n",
        "        return client.sendmail(sender, to, message.as_string())\n",
        "    finally:\n",
        "        client.quit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrIbwkMVoh19"
      },
      "source": [
        "Cela n'aide certainement pas que la fonction crée sa propre instance smtplib.SMTP, qui représente clairement une connexion client SMTP. Si nous avions commencé par les tests, nous y aurions probablement pensé à l'avance et utilisé une inversion mineure de contrôle pour fournir le client SMTP comme argument de fonction. Mais le mal est fait. La fonction send() est utilisée dans toute notre base de code et nous ne voulons pas encore commencer la refactorisation. Il faut d'abord le tester.\n",
        "\n",
        "La fonction send() est stockée dans un module mailer. Nous allons commencer par une approche boîte noire et supposer qu'elle n'a besoin d'aucune configuration. Nous créons un test qui essaie naïvement d'appeler la fonction et espérons réussir. Notre première itération sera la suivante"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5aLf04QpM_Z"
      },
      "source": [
        "#from mailer import send\n",
        "\n",
        "\n",
        "def test_send():\n",
        "  res = send(        \n",
        "      'john.doe@example.com',        \n",
        "      'john.doe@example.com',        \n",
        "      'topic',        \n",
        "      'body'    \n",
        "  )    \n",
        "  assert res == {}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOq7WSgkqOZl"
      },
      "source": [
        "À moins que vous n'ayez un serveur SMTP exécuté localement, vous verrez la sortie suivante lors de l'exécution de pytest :\n",
        "\n",
        "    $ py.test -v --tb line\n",
        "    ======================= test session starts =========================\n",
        "    platform darwin -- Python 3.9.2, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 \n",
        "    -- .../Expert-Python-Programming-Fourth-Edition/.venv/bin/python\n",
        "    cachedir: .pytest_cache\n",
        "    pytest-mutagen-1.3 : Mutations disabled\n",
        "    rootdir: .../Python/\n",
        "    Mocks and unittest.mock module\n",
        "    plugins: mutagen-1.3\n",
        "    collected 1 item\n",
        "    test_mailer.py::test_send FAILED                                [100%]\n",
        "    ============================ FAILURES ===============================\n",
        "    /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.\n",
        "    py:831: ConnectionRefusedError: [Errno 61] Connection refused\n",
        "    ======================= short test summary info =======================\n",
        "    FAILED test_mailer.py::test_send - ConnectionRefusedError: [Errno 61...\n",
        "    ======================== 1 failed in 0.05s ==========================\n",
        "\n",
        "\n",
        "\n",
        "    Le paramètre --tb de la commande py.test peut être utilisé pour contrôler \n",
        "    la longueur de la sortie de trace en cas d'échec du test. Ici, nous avons\n",
        "    utilisé la ligne --tb pour recevoir des retraçages sur une ligne. Les autres \n",
        "    valeurs sont auto, long, short, native et no\n",
        "\n",
        "\n",
        "\n",
        "Voilà nos espoirs. La fonction d'envoi a échoué avec une exception ConnectionRefusedError. Si nous ne voulons pas exécuter le serveur SMTP localement ou envoyer de vrais messages en nous connectant à un vrai serveur SMTP, nous devrons trouver un moyen de remplacer l'implémentation smtplib.SMTP par un faux objet.\n",
        "\n",
        "Afin d'atteindre notre objectif, nous utiliserons deux techniques:\n",
        "\n",
        "* Monkey patching : nous allons modifier le module smtplib à la volée pendant le test afin de tromper la fonction send() en utilisant un faux objet à la place de la classe smtplib.SMTP\n",
        "\n",
        "* Mock d'objet : Nous allons créer un objet faux universel qui peut agir comme un faux pour absolument n'importe quel objet. Nous le ferons juste pour rationaliser notre travail.\n",
        "\n",
        "Avant d'expliquer les deux techniques plus en détail, examinons un exemple de fonction de test: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3KAPw0gsFlr"
      },
      "source": [
        "from unittest.mock import patch\n",
        "\n",
        "def test_send():    \n",
        "  sender = \"john.doe@example.com\"    \n",
        "  to = \"jane.doe@example.com\"    \n",
        "  body = \"Hello jane!\"    \n",
        "  subject = \"How are you?\"    \n",
        "  \n",
        "  with patch('smtplib.SMTP') as mock:        \n",
        "    client = mock.return_value        \n",
        "    client.sendmail.return_value = {}        \n",
        "    res = send(sender, to, subject, body)        \n",
        "    assert client.sendmail.called        \n",
        "    assert client.sendmail.call_args[0][0] == sender        \n",
        "    assert client.sendmail.call_args[0][1] == to        \n",
        "    assert subject in client.sendmail.call_args[0][2]        \n",
        "    assert body in client.sendmail.call_args[0][2]        \n",
        "    assert res == {}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfDUzVcftFGe"
      },
      "source": [
        "Le gestionnaire de contexte unittest.mock.path crée une nouvelle instance de classe unittest.mock.Mock et la remplace sous un chemin d'importation spécifique. Lorsque la fonction send() essaiera d'accéder à l'attribut smtplib.SMTP, elle recevra l'instance mock au lieu de l'objet de classe SMTP\n",
        "\n",
        "Les mock sont assez magiques. Si vous essayez d'accéder à un attribut d'un mock en dehors de l'ensemble de noms réservés par le module unittest.mock, il renverra une nouvelle instance de mock. Les simulations peuvent également être utilisées comme fonctions et, lorsqu'elles sont appelées, renvoient également une nouvelle instance de simulation.\n",
        "\n",
        "La fonction send() s'attend à ce que stmptlib.SMTP soit un objet de type, elle utilisera donc l'appel SMTP() pour obtenir une instance de l'objet client SMTP. Nous utilisons mock.return_value (return_value est l'un des noms réservés) pour obtenir le mock de cet objet client et contrôler la valeur de retour de la méthode client.sendmail().\n",
        "\n",
        "\n",
        "Après l'exécution de la fonction send(), nous avons utilisé quelques autres noms réservés (appelés et call_args) pour vérifier si la méthode client.sendmail() a été appelée et pour inspecter les arguments d'appel.\n",
        "\n",
        "\n",
        "    Notez que ce que nous avons fait ici n'est probablement pas une bonne idée,\n",
        "    car nous venons de retracer ce que fait l'implémentation de la fonction sed().\n",
        "    Vous devriez éviter de le faire dans vos propres tests car il n'y a aucun\n",
        "    but dans un test qui paraphrase simplement l'implémentation de la fonction\n",
        "    testée. Quoi qu'il en soit, il s'agissait plus de présenter les capacités\n",
        "    du module unittest.mock que de montrer comment les tests devraient être écrits\n",
        "\n",
        "\n",
        "Le gestionnaire de contexte patch() du module unittest.mock est un moyen de patcher dynamiquement les chemins d'importation pendant le test. Il peut également être utilisé comme décorateur. C'est une fonctionnalité assez complexe, il n'est donc pas toujours facile de patcher ce que vous voulez. De plus, si vous souhaitez patcher plusieurs objets à la fois, cela nécessitera un peu d'imbrication, ce qui peut être assez gênant.\n",
        "\n",
        "\n",
        "pytest est livré avec un autre moyen d'effectuer un patch de singe. Il est livré avec une fixture Monkeypatch intégrée qui agit comme un proxy de patch. Si nous voudrions réécrire l'exemple précédent avec l'utilisation du fixture monkeypatch, nous pourrions faire ce qui suit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNo4pLvpvE7F"
      },
      "source": [
        "import smtplib\n",
        "\n",
        "def test_send(monkeypatch):\n",
        "    sender = \"john.doe@example.com\"\n",
        "    to = \"jane.doe@example.com\"\n",
        "    body = \"Hello jane!\"\n",
        "    subject = \"How are you?\"\n",
        "\n",
        "    smtp = Mock()\n",
        "    monkeypatch.setattr(smtplib, \"SMTP\", smtp)\n",
        "    client = smtp.return_value\n",
        "    client.sendmail.return_value = {}\n",
        "\n",
        "    res = send(sender, to, subject, body)\n",
        "\n",
        "    assert client.sendmail.called\n",
        "    assert client.sendmail.call_args[0][0] == sender\n",
        "    assert client.sendmail.call_args[0][1] == to\n",
        "    assert subject in client.sendmail.call_args[0][2]\n",
        "    assert body in client.sendmail.call_args[0][2]\n",
        "    assert res == {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6haUJA0hvl7w"
      },
      "source": [
        "Le patch de singe et les mock peuvent être facilement abusés. Cela arrive particulièrement souvent lors de l'écriture de tests après l'implémentation. C'est pourquoi les simulations et les patchs de singe doivent être évités s'il existe d'autres moyens de tester un logiciel de manière fiable. Sinon, vous pourriez vous retrouver avec un projet comportant de nombreux tests qui ne sont que des coquilles vides et ne vérifient pas vraiment l'exactitude du logiciel. Et il n'y a rien de plus dangereux qu'un faux sentiment de sécurité. Il existe également un risque que vos mocks adoptent un comportement différent de la réalité"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YURF5u8Yv9BW"
      },
      "source": [
        "## Couverture de test\n",
        "\n",
        "La couverture de test, également connue sous le nom de couverture de code, est une métrique très utile qui fournit des informations objectives sur la qualité du test d'un code source donné. Il s'agit simplement d'une mesure du nombre et des lignes de code exécutées pendant l'exécution du test. Elle est souvent exprimée en pourcentage, et une couverture à 100% signifie que chaque ligne de code a été exécutée lors des tests.\n",
        "\n",
        "L'outil de couverture de code le plus populaire pour mesurer le code Python est le package coverage  et il est disponible gratuitement sur PyPI. Son utilisation est très simple et ne comporte que deux étapes :\n",
        "\n",
        "* Exécution de la suite de tests à l'aide de l'outil de couverture\n",
        "* Rapporter le rapport de couverture dans le format souhaité\n",
        "\n",
        "La première étape consiste à exécuter la commande d'exécution de couverture dans votre shell avec le chemin d'accès à votre script/programme qui exécute tous les tests. Pour pytest, ça pourrait être quelque chose comme ça.\n",
        "\n",
        "    coverage run $(which pytest)\n",
        "\n",
        "\n",
        "    La commande which est un utilitaire shell utile qui renvoie en sortie\n",
        "    standard le chemin d'accès à l'exécutable de l'autre commande.\n",
        "    L'expression $() peut être utilisée dans de nombreux shells en tant que sous-expression pour\n",
        "     substituer la sortie de la commande dans l'instruction shell donnée en tant que valeur\n",
        "\n",
        "\n",
        "Une autre façon d'invoquer l'exécution de couverture consiste à utiliser l'indicateur -m, qui spécifie le module exécutable. Ceci est similaire à l'invocation de modules exécutables avec python -m. Le package pytest et le module unittest fournissent leurs exécuteurs de test en tant que modules exécutables\n",
        "\n",
        "    $ python -m pytest\n",
        "    $ python -m unittest\n",
        "\n",
        "Ainsi, afin d'exécuter des suites de tests sous la supervision de l'outil de couverture, vous pouvez utiliser les commandes shell suivantes\n",
        "\n",
        "    $ coverage run -m pytest\n",
        "    $ coverage run -m unittest\n",
        "\n",
        "\n",
        "Par défaut, l'outil de couverture mesurera la couverture de test de chaque module importé pendant l'exécution du test. Il peut donc également inclure des packages externes installés dans un environnement virtuel pour votre projet. Vous souhaitez généralement mesurer uniquement la couverture de test du code source de votre propre projet et exclure les sources externes. La commande cover accepte le paramètre --source, qui vous permet de restreindre la mesure à des chemins spéciques comme dans l'exemple suivant.\n",
        "\n",
        "    coverage run --source . -m pytest\n",
        "\n",
        "\n",
        "    L'outil de couverture vous permet de spécifier des indicateurs de configuration dans le fichier setup.cfg.\n",
        "     L'exemple de contenu de setup.cfg pour l'appel d'exécution de couverture ci-dessus serait le suivant:\n",
        "\n",
        "     [coverage:run]\n",
        "     source =    \n",
        "     .\n",
        "\n",
        "\n",
        "Pendant le test, l'outil de couverture créera un fichier .coverage avec les résultats intermédiaires de la mesure de couverture. Après l'exécution, vous pouvez examiner les résultats en exécutant la commande de rapport de couverture.\n",
        "\n",
        "Pour voir comment fonctionne la mesure de couverture en pratique, disons que nous avons décidé de faire une extension ad hoc d'une des classes mentionnées dans la section fixtures du pytest mais que nous n'avons pas pris la peine de la tester correctement. Nous ajouterions la méthode count_keys() à CounterClass comme dans l'exemple suivant :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ISOoYla2xge"
      },
      "source": [
        "from collections import Counter\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "class ViewsStorageBackend(ABC):\n",
        "    @abstractmethod\n",
        "    def increment(self, key: str):\n",
        "        ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def most_common(self, n: int) -> Dict[str, int]:\n",
        "      ...\n",
        "\n",
        "\n",
        "\n",
        "class CounterBackend(ViewsStorageBackend):\n",
        "    def __init__(self):\n",
        "        self._counter = Counter()\n",
        "\n",
        "    def increment(self, key: str):\n",
        "        self._counter[key] += 1\n",
        "\n",
        "    def most_common(self, n: int) -> Dict[str, int]:\n",
        "        return dict(self._counter.most_common(n))\n",
        "\n",
        "    def count_keys(self):\n",
        "        return len(self._counter)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB6FTLln3RPI"
      },
      "source": [
        "Cette méthode count_keys() n'a pas été incluse dans notre déclaration d'interface (la classe de base abstraite ViewsStorageBackend), nous n'avons donc pas anticipé son existence lors de l'écriture de notre suite de tests.\n",
        "\n",
        "Effectuons maintenant un test rapide à l'aide de l'outil de couverture et examinons les résultats globaux. Ceci est l'exemple de transcription du shell montrant ce que nous pourrions potentiellement voir\n",
        "\n",
        "    $ coverage run –source . -m pytest -q\n",
        "\n",
        "    ...............                                                      \n",
        "    \n",
        "    [100%]\n",
        "\n",
        "    16 passed in 0.12s\n",
        "    \n",
        "    $ coverage report -m\n",
        "    \n",
        "    Name               Stmts   Miss  Cover   Missing\n",
        "    ------------------------------------------------\n",
        "    backends.py           21      1    95%   19\n",
        "    interfaces.py          7      0   100%\n",
        "    test_backends.py      39      0   100%\n",
        "    ------------------------------------------------\n",
        "    TOTAL                 67      1    99%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYuDegF44hoV"
      },
      "source": [
        "    Tous les paramètres et indicateurs après le paramètre -m <module> dans la\n",
        "    commande d'exécution de couverture seront transmis directement à\n",
        "    l'invocation du module exécutable. Ici, l'indicateur -q est un indicateur\n",
        "    d'exécution pytest indiquant que nous voulons obtenir un rapport court\n",
        "    (silencieux) de l'exécution du test.\n",
        "\n",
        "\n",
        "Comme on peut le voir, tous les tests ont réussi mais le rapport de couverture a montré que le module backends.py est couvert à 95% par les tests. Cela signifie que 5% des lignes n'ont pas été exécutées pendant le test. Cela met en évidence qu'il y a une lacune dans notre suite de tests.\n",
        "\n",
        "\n",
        "La colonne Manquant (grâce à l'indicateur -m de la commande de rapport de couverture) indique le nombre de lignes qui ont été manquées lors du test. Pour les petits modules avec une couverture élevée, il suffit de localiser les lacunes de couverture manquantes. Lorsque la couverture est très faible, vous souhaiterez probablement un rapport plus approfondi\n",
        "\n",
        "L'outil de couverture est livré avec une commande html de couverture qui générera un rapport de couverture interactif au format HTML.\n",
        "\n",
        "\n",
        "La couverture des tests est une très bonne métrique qui a une forte corrélation avec la qualité globale du code. Les projets avec une faible couverture de test auront statistiquement plus de problèmes de qualité et de défauts. Les projets avec une couverture élevée auront généralement moins de défauts et de problèmes de qualité, en supposant que les tests sont écrits conformément aux bonnes pratiques mises en évidence dans la section des principes de développement piloté par les tests.\n",
        "\n",
        "\n",
        "    Même les projets avec une couverture à 100% peuvent se comporter de manière\n",
        "    imprévisible et être criblés de bugs notoires. Dans de telles situations,\n",
        "    il peut être nécessaire d'utiliser des techniques qui pourraient valider\n",
        "    l'utilité des suites de tests existantes et découvrir les conditions de\n",
        "    test manquées. Une de ces techniques est le test de mutation, discuté dans\n",
        "    la section Test de mutation\n",
        "\n",
        "\n",
        "Pourtant, il est très facile d'écrire des tests dénués de sens qui augmentent considérablement la couverture des tests. Examinez toujours les résultats de la couverture de test des nouveaux projets avec le plus grand soin et ne traitez pas les résultats comme une déclaration définitive de la qualité du code du projet.\n",
        "\n",
        "De plus, la qualité du logiciel ne concerne pas seulement la précision avec laquelle le logiciel est testé, mais aussi sa facilité de lecture, de maintenance et d'extension. Il s'agit donc également de style de code, de conventions communes, de réutilisation de code et de sécurité. Heureusement, la mesure et la validation de ces domaines de programmation peuvent être automatisées dans une certaine mesure.\n",
        "\n",
        "    Dans cette section, nous avons utilisé l'outil de couverture de manière \"classique\".\n",
        "    Si vous utilisez pytest, vous pouvez rationaliser la mesure de la couverture\n",
        "    à l'aide du plug-in pytest-cov, qui peut ajouter automatiquement une\n",
        "    exécution de couverture à chaque exécution de test. Vous pouvez en savoir\n",
        "    plus sur pytest-cov sur https://github.com/pytest-dev/pytest-cov."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMnkF2WC8VHn"
      },
      "source": [
        "## Tests de mutation\n",
        "\n",
        "Avoir une couverture de test à 100% dans votre projet est en effet une chose satisfaisante. Mais plus il est élevé, plus vite vous apprendrez que ce n'est jamais une garantie de logiciel à l'épreuve des balles. D'innombrables projets avec une couverture élevée découvrent de nouveaux bogues dans des parties du code déjà couvertes par des tests. Comment cela se passe-t-il ?\n",
        "\n",
        "Les raisons varient. Parfois, les exigences ne sont pas claires et les tests ne couvrent pas ce qu'ils étaient censés couvrir. Parfois, les tests comportent des erreurs. En fin de compte, les tests ne sont que du code et comme tout autre code sont sensibles aux bugs.\n",
        "\n",
        "Mais parfois, les mauvais tests ne sont que des coquilles vides - ils exécutent certaines unités de code et comparent certains résultats mais ne se soucient pas vraiment de vérifier l'exactitude du logiciel. Et étonnamment, il est plus facile de tomber dans ce piège si vous vous souciez vraiment de la qualité et mesurez la couverture des tests. Ces coquilles vides sont souvent des tests écrits dans la dernière étape juste pour obtenir une couverture parfaite.\n",
        "\n",
        "\n",
        "L'un des moyens de vérifier la qualité des tests est de modifier délibérément le code d'une manière qui, nous le savons, endommagerait définitivement le logiciel et de voir si les tests peuvent découvrir le problème. Si au moins un test échoue, nous sommes sûrs qu'ils sont assez bons pour capturer cette erreur particulière. Si aucun d'entre eux n'échoue, nous devrons peut-être envisager de revoir la suite de tests.\n",
        "\n",
        "\n",
        "Comme les possibilités d'erreurs sont innombrables, il est difficile d'effectuer cette procédure souvent et à plusieurs reprises sans l'aide d'outils et de méthodologies spéciques. Une de ces méthodologies est le test de mutation\n",
        "\n",
        "\n",
        "Les tests de mutation reposent sur l'hypothèse que la plupart des défauts du logiciel sont introduits par de petites erreurs telles que des erreurs un par un, des opérateurs de comparaison inversés, des plages erronées, etc. Il y a aussi l'hypothèse que ces petites erreurs se transforment en fautes plus importantes qui devraient être reconnaissables par des tests.\n",
        "\n",
        "\n",
        "Les tests de mutation utilisent des opérateurs de modification bien définis connus sous le nom de mutations qui simulent des erreurs de programmeur petites et typiques. Des exemples de ceux-ci peuvent être.\n",
        "\n",
        "* Remplacement de l'opérateur == par l'opérateur is\n",
        "* Remplacer un littéral 0 par 1\n",
        "* Commutation des opérandes de l'opérateur <\n",
        "* Ajouter un suffixe à un littéral de chaîne\n",
        "* Remplacement d'une instruction break par continue\n",
        "\n",
        "\n",
        "Dans chaque série de tests de mutation, le programme original est légèrement modifié pour produire un soi-disant mutant. Si le mutant peut réussir tous les tests, on dit qu'il a survécu au test. Si au moins un des tests a échoué, on dit qu'il a été tué pendant le test. Le but des tests de mutation est de renforcer la suite de tests afin qu'elle ne permette pas à de nouveaux mutants de survivre.\n",
        "\n",
        "Toute cette théorie peut sembler un peu vague à ce stade, nous allons donc maintenant examiner un exemple pratique d'une session de test de mutation. Nous allons essayer de tester une fonction is_prime() censée vérifier si un nombre entier est un nombre premier ou non.\n",
        "\n",
        "Un nombre premier est un nombre naturel supérieur à 1 qui n'est divisible que par lui-même et 1. Nous ne voulons pas nous répéter, il n'y a donc pas de moyen simple de tester la fonction is_prime() autre que de fournir des exemples de données. Nous allons commencer par le test simple suivant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEytfSa0-oe-"
      },
      "source": [
        "\n",
        "def test_primes_true():\n",
        "    assert is_prime(5)\n",
        "    assert is_prime(7)\n",
        "\n",
        "\n",
        "def test_primes_false():\n",
        "    assert not is_prime(4)\n",
        "    assert not is_prime(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxZEjfIG-1Fl"
      },
      "source": [
        "Nous pourrions utiliser un peu de paramétrage, mais laissons cela pour plus tard. Enregistrons cela dans le fichier test_primes.py et passons à la fonction is_prime(). Ce qui nous intéresse en ce moment, c'est la simplicité, nous allons donc créer une implémentation très naïve comme suit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOp0Bn8O_AiF"
      },
      "source": [
        "def is_prime(number):\n",
        "    if not isinstance(number, int) or number < 0:\n",
        "        return False\n",
        "    \n",
        "    if number in (0, 1):        \n",
        "      return False\n",
        "\n",
        "    for element in range(2, number):\n",
        "        if number % element == 0:\n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "551i_4iK_S02"
      },
      "source": [
        "Ce n'est peut-être pas l'implémentation la plus performante, mais c'est très simple et devrait donc être facile à comprendre. Seuls les entiers supérieurs à 1 peuvent être premiers. Nous commençons par vérifier le type et par rapport aux valeurs 0 et 1. Pour les autres nombres, nous itérons sur des nombres entiers inférieurs à nombre et supérieurs à 1. Si nombre n'est divisible par aucun de ces nombres entiers, cela signifie qu'il s'agit d'un nombre premier. Enregistrons cette fonction dans le fichier primes.py\n",
        "\n",
        "\n",
        "Il est maintenant temps d'évaluer la qualité de nos tests. Il existe quelques outils de test de mutation disponibles sur PyPI. Celui qui semble le plus simple à utiliser est **mutmut**, et nous l'utiliserons lors de notre session de test de mutation. mutmut vous oblige à définir une configuration mineure qui lui indique comment les tests sont exécutés et comment faire muter votre code. Il utilise sa propre section [mutmut] dans le fichier setup.cfg commun. Notre configuration sera la suivante\n",
        "\n",
        "    [mutmut]\n",
        "    paths_to_mutate=primes.py\n",
        "    runner=python -m pytest -x\n",
        "\n",
        "La variable paths_to_mutate spécifie les chemins des fichiers source que mutmut est capable de muter. Les tests de mutation dans les grands projets peuvent prendre beaucoup de temps, il est donc crucial de guider mutmut sur ce qu'il est censé muter, juste pour gagner du \n",
        "\n",
        "\n",
        "La variable runner spécifie la commande utilisée pour exécuter les tests. mutmut est indépendant du framework, il prend donc en charge tout type de framework de test doté d'un exécutable runner en tant que commande shell. Ici, nous utilisons pytest avec le drapeau -x. Ce drapeau indique à pytest d'abandonner le test au premier échec. Les tests de mutation consistent à découvrir des mutants survivants. Si l'un des tests échoue, nous saurons déjà que le mutant n'a pas survécu\n",
        "\n",
        "Il est maintenant temps de commencer la session de test de mutation. L'utilisation de l'outil mutmut est très similaire à celle de l'outil de couverture, notre travail commence donc par la sous-commande \n",
        "\n",
        "    mutmut run\n",
        "\n",
        "\n",
        "La course entière prendra quelques secondes. Une fois que mutmut aura terminé la validation des mutants, nous verrons le résumé suivant du run\n",
        "\n",
        "\n",
        "      - Mutation testing starting -\n",
        "      \n",
        "      \n",
        "      These are the steps:\n",
        "      1. A full test suite run will be made to make sure we   \n",
        "          can run the tests successfully and we know how long   \n",
        "          it takes (to detect infinite loops for example)\n",
        "      2. Mutants will be generated and checked\n",
        "      \n",
        "      Results are stored in .mutmut-cache.\n",
        "      Print found mutants with `mutmut results`.\n",
        "\n",
        "\n",
        "      Legend for output: \n",
        "        Killed mutants.   The goal is for everything to end up in this \n",
        "      bucket.\n",
        "      ⏰  Timeout.         Test suite took 10 times as long as the baseline so were killed.\n",
        "        Suspicious.       Tests took a long time, but not long enough to be fatal.\n",
        "        Survived.         This means your tests needs to be expanded.\n",
        "        Skipped.          Skipped.\n",
        "      \n",
        "      1. Running tests without mutations \n",
        "      Running...Done\n",
        "      2. Checking mutants \n",
        "      15/15    8  ⏰  0    0    7    0\n",
        "\n",
        "\n",
        "La dernière ligne montre un bref résumé des résultats. Nous pouvons obtenir une vue détaillée en exécutant la commande mutmut results. Nous avons obtenu la sortie suivante dans notre session\n",
        "\n",
        "\n",
        "    $ mutmut results\n",
        "    To apply a mutant on disk:    \n",
        "        mutmut apply <id>\n",
        "    \n",
        "    \n",
        "    To show a mutant:    \n",
        "        mutmut show <id>\n",
        "    \n",
        "    Survived   (7)\n",
        "    \n",
        "    ---- primes.py (7) ----\n",
        "    \n",
        "    8-10, 12-15\n",
        "\n",
        "\n",
        "La dernière ligne montre les identifiants des mutants qui ont survécu au test. Nous pouvons voir que 7 mutants ont survécu et que leurs identifiants sont compris entre 8-10 et 12-15. La sortie affiche également des informations utiles sur la façon d'examiner les mutants à l'aide de la commande mutmut show <id>. Vous pouvez également examiner les mutants en bloc en utilisant le nom du fichier source comme valeur <id>\n",
        "\n",
        "Nous le faisons uniquement à des fins d'illustration, nous ne passerons donc en revue que deux mutants. Regardons le premier avec un ID de 8\n",
        "\n",
        "    $ mutmut show 8\n",
        "    --- primes.py\n",
        "    +++ primes.py\n",
        "    @@ -2,7 +2,7 @@     \n",
        "        if not isinstance(number, int) or number < 0:\n",
        "          return False\n",
        "\n",
        "    -    if number in (0, 1):\n",
        "    +    if number in (1, 1):         \n",
        "            return False     \n",
        "          \n",
        "          \n",
        "          for element in range(2, number):\n",
        "\n",
        "mutmut a modifié les valeurs de plage de notre nombre if dans (...) et nos tests n'ont clairement pas détecté le problème. Cela signifie que nous devons probablement inclure ces valeurs dans nos conditions de test.\n",
        "\n",
        "Jetons maintenant un œil au dernier mutant avec un ID de 15 :\n",
        "\n",
        "    $ mutmut show 15\n",
        "    --- primes.py\n",
        "    +++ primes.py\n",
        "    @@ -1,6 +1,6 @@ \n",
        "    def is_prime(number):     \n",
        "        if not isinstance(number, int) or number < 0:\n",
        "    -        return False\n",
        "    +        return True     \n",
        "    \n",
        "        if number in (0, 1):         \n",
        "            return False\n",
        "\n",
        "mutmut a inversé la valeur du littéral booléen après les vérifications du type et de la plage de valeurs. Le mutant a survécu car nous avons inclus une vérification de type mais n'avons pas testé ce qui se passe lorsque la valeur d'entrée a le mauvais type.\n",
        "\n",
        "Dans notre cas, tous ces mutants auraient pu être tués si nous avions inclus plus d'échantillons de test dans nos tests. Si nous étendions la suite de tests pour couvrir plus de cas particuliers et de valeurs invalides, cela la rendrait probablement plus robuste. Voici un ensemble de tests révisé :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7HHBufEEuEe"
      },
      "source": [
        "def test_primes_true():\n",
        "    assert is_prime(2)\n",
        "    assert is_prime(5)\n",
        "    assert is_prime(7)\n",
        "\n",
        "\n",
        "def test_primes_false():\n",
        "    assert not is_prime(-200)\n",
        "    assert not is_prime(3.1)\n",
        "    assert not is_prime(0)\n",
        "    assert not is_prime(1)\n",
        "    assert not is_prime(4)\n",
        "    assert not is_prime(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8HAExev_2fv"
      },
      "source": [
        "Les tests de mutation sont une méthodologie hybride car ils vérifient non seulement la qualité des tests, mais peuvent également mettre en évidence du code potentiellement redondant. Par exemple, si nous implémentons les améliorations de test de l'exemple ci-dessus, nous verrons toujours deux mutants survivants\n",
        "\n",
        "\n",
        "    # mutant 12\n",
        "    --- primes.py\n",
        "    +++ primes.py\n",
        "    @@ -1,5 +1,5 @@ \n",
        "     def is_prime(number):\n",
        "     -    if not isinstance(number, int) or number < 0:\n",
        "     +    if not isinstance(number, int) or number <= 0:         \n",
        "              return False     \n",
        "          \n",
        "          if number in (0, 1):\n",
        "    \n",
        "    # mutant 13\n",
        "    ---primes.py\n",
        "    +++ primes.py\n",
        "    @@ -1,5 +1,5 @@ \n",
        "    def is_prime(number):\n",
        "    -    if not isinstance(number, int) or number < 0:\n",
        "    +    if not isinstance(number, int) or number < 1:         \n",
        "            return False     \n",
        "            \n",
        "          \n",
        "        if number in (0, 1):\n",
        "\n",
        "      \n",
        "Ces deux mutants survivent car les deux clauses if que nous avons utilisées peuvent potentiellement gérer la même condition. Cela signifie que le code que nous avons écrit est probablement trop complexe et peut être simplifié. Nous pourrons tuer ces deux mutants exceptionnels si nous réduisons deux instructions if en une seule :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6znzPeIuGzWh"
      },
      "source": [
        "def is_prime(number):\n",
        "    if not isinstance(number, int) or number <= 1:\n",
        "        return False\n",
        "\n",
        "    for element in range(2, number):\n",
        "        if number % element == 0:\n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yLxqcz5G3ZB"
      },
      "source": [
        "Le test de mutation est une technique vraiment intéressante qui peut renforcer la qualité des tests. Un problème est qu'il ne s'adapte pas bien. Sur des projets plus importants, le nombre de mutants potentiels sera vraiment important et pour les valider, vous devez exécuter toute la suite de tests. Il faudra beaucoup de temps pour exécuter une seule session de mutation si vous avez de nombreux tests de longue durée. C'est pourquoi les tests de mutation fonctionnent bien avec des tests unitaires simples, mais sont très limités en ce qui concerne les tests d'intégration. Pourtant, c'est un excellent outil pour percer des trous dans ces suites de tests de couverture parfaites.\n",
        "\n",
        "Au cours des dernières sections, nous nous sommes concentrés sur des outils et des approches systématiques pour la rédaction de tests et l'automatisation de la qualité. Ces approches systématiques créent une bonne base pour vos opérations de test, mais ne garantissent pas que vous serez efficace dans la rédaction des tests ou que les tests seront faciles. Les tests peuvent parfois être fastidieux et ennuyeux. Ce qui le rend plus amusant, c'est la grande collection d'utilitaires disponibles sur PyPI qui vous permet de réduire les pièces ennuyeuses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDI0nImZHhrg"
      },
      "source": [
        "## Utilitaires de test utiles\n",
        "\n",
        "Lorsqu'il s'agit d'efficacité dans la rédaction de tests, cela se résume généralement à la gestion de tous ces problèmes banals ou gênants, tels que la fourniture d'entrées de données réalistes, la gestion de traitements urgents ou le travail avec des services distants. Les programmeurs expérimentés augmentent généralement leur efficacité à l'aide d'une large collection de petits outils pour faire face à tous ces petits problèmes typiques. Jetons un coup d'oeil à quelques-uns d'entre eux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2LXLoqKH043"
      },
      "source": [
        "## Faking  des valeurs de données réalistes.\n",
        "\n",
        "Lors de l'écriture de tests basés sur des échantillons de données d'entrée-sortie, nous devons souvent fournir des valeurs qui ont une certaine signification dans notre application.\n",
        "\n",
        "* Noms de personnes\n",
        "* Adresses\n",
        "* Numéros de téléphone\n",
        "* Adresses e-mail\n",
        "* Numéros d'identification tels que les identifiants fiscaux ou de sécurité sociale\n",
        "\n",
        "\n",
        "Le moyen le plus simple de contourner cela est d'utiliser des valeurs codées en dur. Nous l'avons déjà fait dans l'exemple de notre fonction test_send() dans la section Mocks and unittest.mock module.\n",
        "\n",
        "    def test_send():    \n",
        "    sender = \"john.doe@example.com\"    \n",
        "    to = \"jane.doe@example.com\"    \n",
        "    body = \"Hello jane!\"    \n",
        "    subject = \"How are you?\"    \n",
        "    ...\n",
        "\n",
        "L'avantage de faire cela est que quiconque lit le test sera capable de comprendre visuellement les valeurs, cela peut donc également servir à des fins de documentation de test. Mais le problème de l'utilisation de valeurs codées en dur est qu'elle ne permet pas aux tests de rechercher efcacement dans le vaste espace des erreurs potentielles. Nous avons déjà vu dans la section Tests de mutation comment un petit ensemble d'échantillons de test peut conduire à des tests de mauvaise qualité et à un faux sentiment de sécurité quant à la qualité de votre code.\n",
        "\n",
        "Nous pourrions bien sûr résoudre ce problème en paramétrant des tests et en utilisant des échantillons de données beaucoup plus réalistes. Mais c'est beaucoup de travail répétitif banal et de nombreux développeurs ne sont pas disposés à le faire à plus grande échelle\n",
        "\n",
        "Une façon de contourner cette monotonie des ensembles de données d'échantillon est d'utiliser un générateur d'entrées de données facilement disponible qui pourrait fournir des valeurs réalistes. L'un de ces générateurs est le package faker disponible sur PyPI. faker est livré avec un plugin pytest intégré, qui fournit une structure de faker qui peut être facilement utilisée dans n'importe lequel de vos tests. Ce qui suit est la partie modifiée de la fonction test_send() qui utilise le faux fixture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEnk2Td8NXts"
      },
      "source": [
        "from faker import Faker\n",
        "\n",
        "def test_send(faker: Faker):    \n",
        "  sender = faker.email()    \n",
        "  to = faker.email()    \n",
        "  body = faker.paragraph()    \n",
        "  subject = faker.sentence()    \n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLGBxm9PNiCH"
      },
      "source": [
        "À chaque exécution, le faussaire ensemencera le test avec différents échantillons de données. Grâce à cela, vous êtes plus susceptible de découvrir des problèmes potentiels. De plus, si vous souhaitez exécuter les mêmes tests plusieurs fois en utilisant différentes valeurs aléatoires, vous pouvez utiliser l'astuce de paramétrage pytest suivante :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrZ70pVnNq83"
      },
      "source": [
        "import pytest\n",
        "\n",
        "@pytest.mark.parametrize(\"iteration\", range(10))\n",
        "def test_send(faker: Faker, iteration: int):    \n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mx4aql9NyzB"
      },
      "source": [
        "pytest a des dizaines de classes de fournisseurs de données et chacune a plusieurs méthodes de saisie de données. Chaque méthode peut être obtenue directement via l'instance de classe Faker. Il prend également en charge la localisation, de sorte que de nombreuses classes de fournisseurs sont disponibles dans des versions pour différentes langues.\n",
        "\n",
        "faker peut également fournir des entrées de date et d'heure dans diverses normes. Ce qu'il ne peut pas faire, c'est geler le temps. Mais ne vous inquiétez pas, nous avons un package différent pour cela\n",
        "\n"
      ]
    }
  ]
}